{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "O campo da física de altas energias busca compreender as quatro principais forças que regem as interações entre partículas. As informações necessárias para este estudo são coletadas a partir do estudo de uma classificação específica de partículas, que são produzidas em condições extremas.\n",
    "\n",
    "A única forma conhecida de se gerar e analisar tais partículas é por meio de colisões realizadas em aceleradores de partículas, porém nem toda colisão gera dados relevantes. Na realidade, o Grande Colisor de Hadrons (LHC) gera em média três partículas de interesse (denominadas partículas exóticas) a cada $10^9$ colisões. As colisões restantes geram partículas denominadas background, e não são utilizadas.\n",
    "\n",
    "Dada a quantidade de colisões realizada por hora, e a complexidade de se analisar os dados obtidos, há um grande interesse em desenvolver uma forma automatizada de se diferenciar partículas exóticas das background. Para realizar esta diferenciação, são utilizados 18 dados. Destes, 8 são dados cinéticos coletados por detectores dentro do próprio acelerador de partículas, enquanto os 10 restantes são calculados em função dos anteriores.\n",
    "\n",
    "O dataset analizado, denominado SUSY, apresenta um total de cinco milhões de instâncias de partículas teóricas. Cada uma possui 18 variáveis numéricas representando os dados utilizados para a diferenciação entre partículas exóticas e background, e uma variável booleana que representa a **qual destas classificações** a partícula em questão se encaixa. Os dados do dataset SUSY foram gerados a partir de simulações Monte Carlo, ou seja, não são produto de colisões reais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos\n",
    "## Geral\n",
    "Leonardo\n",
    "    Determinar modelo de predição para classificar se a particula é exotica ou não.\n",
    "## Especificos\n",
    "Leonardo\n",
    "    Determinar a melhor metrica para avaliar os resultados exemplos (precisão, acurácia, falso positivo, falso negativo) e porque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informações do Dataset\n",
    "info, head, tail, describe\n",
    "Akiyoshi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de Classificação\n",
    "\n",
    "KNN,SGD,KA,GBC colocar imagem do sklearnin de seleção dos algoritmos  \n",
    "Rafael, Adrianne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise das Features\n",
    "Gesiel falar sobre as variações dos modelos e pré concluir\n",
    "## Pré Processamento\n",
    "    O pré processamento de dados é uma fase no aprendizado de maquina que busca avaliar a qualidade dos dados e suas relações, e se iniciar logo depois de os dados terem sido coletados e organizados em um conjunto de dados. Essa fase se dá pelo fato de existirem diversos problemas nos dados que precisam ser solucionados como por exemplo: dados faltando, dados corrompidos, dados irrelevantes e valores desconhecidos.\n",
    "    \n",
    "    Uma caracteristica que os algoritmos de AM, vem assumindo é determinar que os dados já vem pré processados, isso porque a comunidade de AM usam os repositorios de dados para obter conjuntos de dados para utilização e por essa razão não apresentam varios problemas que podem ser encontrados em dados coletados no mundo real e acabam não realizando a fase de pré processamento.\n",
    "    \n",
    "    Diante desde dilema e pelo fato de o dataset SUSY possuir um conjunto de dados de cinco milhoes de registro e 18 variaveis independentes, esta sessão possue por objetivo buscar reduzir as variaveis independentes(features) do dataset, afim de melhorar o processamento e os resultados obtidos após o processamento e analise dos resultados, dessa forma varias tecnicas de redução de featues juntamente com analise gráfica seram utiladas em conjunto para determinar se é possivel reduzir as features.\n",
    "    \n",
    "    Este objetivo foi proposto depois de uma analise da estrutura dos dados e visualização dos mesmos utilizando tecnicas de visualização de dados, foi observado que os dados já foram pré processados como pode ser observado: não existem dados faltando, não existem dados corrompidos e não existem valores corrompidos.\n",
    "    \n",
    "    Analisando dois trabalhos feitos utilizando este mesmo conjunto de dados(SUSY) disponíveis em: [Searching for Exotic Particles in ...](https://arxiv.org/pdf/1402.4735.pdf) e [Modelo preditivo de Susy ...](http://machinelearningandspark.blogspot.com.br/2016/07/susy-dataset.html), notou-se que os melhores resultados foram obtidos utilizando todas as features\n",
    "    \n",
    "    \n",
    "## Redução de Features\n",
    "Afonso, João Pedro\n",
    "## Teste Redução\n",
    "    Testar as features com os algoritmos SGD, GBC de classificação\n",
    "    Afonso\n",
    "## Conclusão Análise Features\n",
    "    Afonso\n",
    "Concluir que é melhor não reduzir as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "## Algoritmo utilizado\n",
    "Para a Solução do Problema, a equipe fez o uso de 3 algorítmos diferentes. \n",
    "\n",
    "A primeira abordagem que tomamos para a resolução do problema, foi implementar o *Nearest Neighbors Classification*. Durante a aplicação do algorítmo KNN, observamos certa demora na execução do código, entretanto, ao final do teste, foi possível obter, com 17 vizinhos e 500.000 amostras, o resultado ótimo para o algorítmo, que foi uma acurácia de 0.78437066, obtida após a feature selection, sendo a acurácia, utilizando todas as features, equivalente a cerca de 0.76. \n",
    "\n",
    "Entretanto, após consulta do site da biblioteca Sklearn e também análise da ineficiência do algoritmo, por parte da equipe, concluiu-se que o uso do algorítmo KNN era inedequado para uma base de dados da dimensão da nossa, visto que, este algorítmo, é adequado para um máximo de 100.000 amostras e nossa base tinha 5.000.000.\n",
    "\n",
    "Visto a ineficiência do KNN, apesar do resultado razoável, decidimos fazer uso do *Stochastic Gradient Descent*. Com o uso do algorítmo SGD, além de reduzir drasticamente o tempo de execução do algorítmo, reduzindo o tempo de varios minutos para segundos, foi possível obter um resultado melhor ainda! A acurácia aumentou para 0.7884, utilizando todas as features do dataset, apesar do resultado, não foram feitas alterações posteriores ou a aplicação da feature selection com o SGD.\n",
    "\n",
    "Apesar de já ter obtido bons resultados utilizando o SGD, a equipe também implementou o *Gradient Boosting* ou *Gradient Boosted Regression Trees Classifier*. O algorítmo GBRT melhorou ainda mais o resultado obtido pela equipe, tendo, ao final, com as 5 milhões de amostras, dataset de treino de 75% e min_samples_split de 700, obter o melhor resultado. Sendo esta, uma acurácia total e final de 0.8031416 nos testes.\n",
    "\n",
    "## Resultados Obtidos\n",
    "Thiago\n",
    "    Apresentar resultados obtidos, gráficos e resultados da mensuração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "BATISTA, Gustavo Enrique de Almeida Prado Alves. Pré-processamento de dados em aprendizado de máquina supervisionado. 2003. Tese (Doutorado em Ciências de Computação e Matemática Computacional) - Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, 2003. doi:10.11606/T.55.2003.tde-06102003-160219. Acesso em: 2018-04-16."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
