{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "O campo da física de altas energias busca compreender as quatro principais forças que regem as interações entre partículas. As informações necessárias para este estudo são coletadas a partir do estudo de uma classificação específica de partículas, que são produzidas em condições extremas.\n",
    "\n",
    "A única forma conhecida de se gerar e analisar tais partículas é por meio de colisões realizadas em aceleradores de partículas, porém nem toda colisão gera dados relevantes. Na realidade, o Grande Colisor de Hadrons (LHC) gera em média três partículas de interesse (denominadas partículas exóticas) a cada $10^9$ colisões. As colisões restantes geram partículas denominadas background, e não são utilizadas.\n",
    "\n",
    "Dada a quantidade de colisões realizada por hora, e a complexidade de se analisar os dados obtidos, há um grande interesse em desenvolver uma forma automatizada de se diferenciar partículas exóticas das background. Para realizar esta diferenciação, são utilizados 18 dados. Destes, 8 são dados cinéticos coletados por detectores dentro do próprio acelerador de partículas, enquanto os 10 restantes são calculados em função dos anteriores.\n",
    "\n",
    "O dataset analizado, denominado SUSY, apresenta um total de cinco milhões de instâncias de partículas teóricas. Cada uma possui 18 variáveis numéricas representando os dados utilizados para a diferenciação entre partículas exóticas e background, e uma variável booleana que representa a **qual destas classificações** a partícula em questão se encaixa. Os dados do dataset SUSY foram gerados a partir de simulações Monte Carlo, ou seja, não são produto de colisões reais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos\n",
    "## Geral\n",
    "Leonardo\n",
    "    Determinar modelo de predição para classificar se a particula é exotica ou não.\n",
    "## Especificos\n",
    "Leonardo\n",
    "    Determinar a melhor metrica para avaliar os resultados exemplos (precisão, acurácia, falso positivo, falso negativo) e porque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informações do Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando bibliotecas para manipulação do Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando Dataset, e separando entre variáveis dependentes e independentes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o dataset é muito extenso, será utilizada a técnica do **memmap** que são usados para acessar pequenos segmentos de arquivos grandes no disco, sem carregar todo o arquivo na memória. [(NUMPY_MEMMAP)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.memmap('data_x.mymemmap', dtype='float32', mode='w+', shape=(4999999,18))\n",
    "a[:] = pd.read_csv('SUSY.csv').iloc[:,1:19].values\n",
    "del a\n",
    "data_x = np.memmap('data_x.mymemmap', dtype='float32', mode='r+', shape=(4999999,18))\n",
    "\n",
    "b = np.memmap('data_y.mymemmap', dtype='float32', mode='w+', shape=(4999999))\n",
    "b[:] = pd.read_csv('SUSY.csv').iloc[:,0].values\n",
    "del b\n",
    "data_y = np.memmap('data_y.mymemmap', dtype='float32', mode='r+', shape=(4999999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, separando as variáveis entre dataset de treino e dataset de teste, utilizando o train_test_split [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) do scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x,data_y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando o Dataset agora. Para tal, temos que importá-lo na memória. Após análise básica, podemos deletar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('SUSY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "      <th>9.728614687919616699e-01</th>\n",
       "      <th>6.538545489311218262e-01</th>\n",
       "      <th>1.176224589347839355e+00</th>\n",
       "      <th>1.157156467437744141e+00</th>\n",
       "      <th>-1.739873170852661133e+00</th>\n",
       "      <th>-8.743090629577636719e-01</th>\n",
       "      <th>5.677649974822998047e-01</th>\n",
       "      <th>-1.750000417232513428e-01</th>\n",
       "      <th>8.100607395172119141e-01</th>\n",
       "      <th>-2.525521218776702881e-01</th>\n",
       "      <th>1.921887040138244629e+00</th>\n",
       "      <th>8.896374106407165527e-01</th>\n",
       "      <th>4.107718467712402344e-01</th>\n",
       "      <th>1.145620822906494141e+00</th>\n",
       "      <th>1.932632088661193848e+00</th>\n",
       "      <th>9.944640994071960449e-01</th>\n",
       "      <th>1.367815494537353516e+00</th>\n",
       "      <th>4.071449860930442810e-02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.667973</td>\n",
       "      <td>0.064191</td>\n",
       "      <td>-1.225171</td>\n",
       "      <td>0.506102</td>\n",
       "      <td>-0.338939</td>\n",
       "      <td>1.672543</td>\n",
       "      <td>3.475464</td>\n",
       "      <td>-1.219136</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>3.775174</td>\n",
       "      <td>1.045977</td>\n",
       "      <td>0.568051</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448410</td>\n",
       "      <td>0.205356</td>\n",
       "      <td>1.321893</td>\n",
       "      <td>0.377584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444840</td>\n",
       "      <td>-0.134298</td>\n",
       "      <td>-0.709972</td>\n",
       "      <td>0.451719</td>\n",
       "      <td>-1.613871</td>\n",
       "      <td>-0.768661</td>\n",
       "      <td>1.219918</td>\n",
       "      <td>0.504026</td>\n",
       "      <td>1.831248</td>\n",
       "      <td>-0.431385</td>\n",
       "      <td>0.526283</td>\n",
       "      <td>0.941514</td>\n",
       "      <td>1.587535</td>\n",
       "      <td>2.024308</td>\n",
       "      <td>0.603498</td>\n",
       "      <td>1.562374</td>\n",
       "      <td>1.135454</td>\n",
       "      <td>0.180910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381256</td>\n",
       "      <td>-0.976145</td>\n",
       "      <td>0.693152</td>\n",
       "      <td>0.448959</td>\n",
       "      <td>0.891753</td>\n",
       "      <td>-0.677328</td>\n",
       "      <td>2.033060</td>\n",
       "      <td>1.533041</td>\n",
       "      <td>3.046260</td>\n",
       "      <td>-1.005285</td>\n",
       "      <td>0.569386</td>\n",
       "      <td>1.015211</td>\n",
       "      <td>1.582217</td>\n",
       "      <td>1.551914</td>\n",
       "      <td>0.761215</td>\n",
       "      <td>1.715464</td>\n",
       "      <td>1.492257</td>\n",
       "      <td>0.090719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.309996</td>\n",
       "      <td>-0.690089</td>\n",
       "      <td>-0.676259</td>\n",
       "      <td>1.589283</td>\n",
       "      <td>-0.693326</td>\n",
       "      <td>0.622907</td>\n",
       "      <td>1.087562</td>\n",
       "      <td>-0.381742</td>\n",
       "      <td>0.589204</td>\n",
       "      <td>1.365479</td>\n",
       "      <td>1.179295</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.728563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083158</td>\n",
       "      <td>0.043429</td>\n",
       "      <td>1.154854</td>\n",
       "      <td>0.094859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.456398</td>\n",
       "      <td>1.099371</td>\n",
       "      <td>1.512453</td>\n",
       "      <td>0.751772</td>\n",
       "      <td>0.638967</td>\n",
       "      <td>-0.742216</td>\n",
       "      <td>0.322601</td>\n",
       "      <td>1.321054</td>\n",
       "      <td>0.169502</td>\n",
       "      <td>0.359941</td>\n",
       "      <td>0.489256</td>\n",
       "      <td>0.416168</td>\n",
       "      <td>0.754829</td>\n",
       "      <td>0.303750</td>\n",
       "      <td>0.461067</td>\n",
       "      <td>0.345541</td>\n",
       "      <td>0.733242</td>\n",
       "      <td>0.186044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.000000000000000000e+00  9.728614687919616699e-01  \\\n",
       "0                       1.0                  1.667973   \n",
       "1                       1.0                  0.444840   \n",
       "2                       1.0                  0.381256   \n",
       "3                       1.0                  1.309996   \n",
       "4                       0.0                  0.456398   \n",
       "\n",
       "   6.538545489311218262e-01  1.176224589347839355e+00  \\\n",
       "0                  0.064191                 -1.225171   \n",
       "1                 -0.134298                 -0.709972   \n",
       "2                 -0.976145                  0.693152   \n",
       "3                 -0.690089                 -0.676259   \n",
       "4                  1.099371                  1.512453   \n",
       "\n",
       "   1.157156467437744141e+00  -1.739873170852661133e+00  \\\n",
       "0                  0.506102                  -0.338939   \n",
       "1                  0.451719                  -1.613871   \n",
       "2                  0.448959                   0.891753   \n",
       "3                  1.589283                  -0.693326   \n",
       "4                  0.751772                   0.638967   \n",
       "\n",
       "   -8.743090629577636719e-01  5.677649974822998047e-01  \\\n",
       "0                   1.672543                  3.475464   \n",
       "1                  -0.768661                  1.219918   \n",
       "2                  -0.677328                  2.033060   \n",
       "3                   0.622907                  1.087562   \n",
       "4                  -0.742216                  0.322601   \n",
       "\n",
       "   -1.750000417232513428e-01  8.100607395172119141e-01  \\\n",
       "0                  -1.219136                  0.012955   \n",
       "1                   0.504026                  1.831248   \n",
       "2                   1.533041                  3.046260   \n",
       "3                  -0.381742                  0.589204   \n",
       "4                   1.321054                  0.169502   \n",
       "\n",
       "   -2.525521218776702881e-01  1.921887040138244629e+00  \\\n",
       "0                   3.775174                  1.045977   \n",
       "1                  -0.431385                  0.526283   \n",
       "2                  -1.005285                  0.569386   \n",
       "3                   1.365479                  1.179295   \n",
       "4                   0.359941                  0.489256   \n",
       "\n",
       "   8.896374106407165527e-01  4.107718467712402344e-01  \\\n",
       "0                  0.568051                  0.481928   \n",
       "1                  0.941514                  1.587535   \n",
       "2                  1.015211                  1.582217   \n",
       "3                  0.968218                  0.728563   \n",
       "4                  0.416168                  0.754829   \n",
       "\n",
       "   1.145620822906494141e+00  1.932632088661193848e+00  \\\n",
       "0                  0.000000                  0.448410   \n",
       "1                  2.024308                  0.603498   \n",
       "2                  1.551914                  0.761215   \n",
       "3                  0.000000                  1.083158   \n",
       "4                  0.303750                  0.461067   \n",
       "\n",
       "   9.944640994071960449e-01  1.367815494537353516e+00  \\\n",
       "0                  0.205356                  1.321893   \n",
       "1                  1.562374                  1.135454   \n",
       "2                  1.715464                  1.492257   \n",
       "3                  0.043429                  1.154854   \n",
       "4                  0.345541                  0.733242   \n",
       "\n",
       "   4.071449860930442810e-02  \n",
       "0                  0.377584  \n",
       "1                  0.180910  \n",
       "2                  0.090719  \n",
       "3                  0.094859  \n",
       "4                  0.186044  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4999999 entries, 0 to 4999998\n",
      "Data columns (total 19 columns):\n",
      "0.000000000000000000e+00     float64\n",
      "9.728614687919616699e-01     float64\n",
      "6.538545489311218262e-01     float64\n",
      "1.176224589347839355e+00     float64\n",
      "1.157156467437744141e+00     float64\n",
      "-1.739873170852661133e+00    float64\n",
      "-8.743090629577636719e-01    float64\n",
      "5.677649974822998047e-01     float64\n",
      "-1.750000417232513428e-01    float64\n",
      "8.100607395172119141e-01     float64\n",
      "-2.525521218776702881e-01    float64\n",
      "1.921887040138244629e+00     float64\n",
      "8.896374106407165527e-01     float64\n",
      "4.107718467712402344e-01     float64\n",
      "1.145620822906494141e+00     float64\n",
      "1.932632088661193848e+00     float64\n",
      "9.944640994071960449e-01     float64\n",
      "1.367815494537353516e+00     float64\n",
      "4.071449860930442810e-02     float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 724.8 MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "      <th>9.728614687919616699e-01</th>\n",
       "      <th>6.538545489311218262e-01</th>\n",
       "      <th>1.176224589347839355e+00</th>\n",
       "      <th>1.157156467437744141e+00</th>\n",
       "      <th>-1.739873170852661133e+00</th>\n",
       "      <th>-8.743090629577636719e-01</th>\n",
       "      <th>5.677649974822998047e-01</th>\n",
       "      <th>-1.750000417232513428e-01</th>\n",
       "      <th>8.100607395172119141e-01</th>\n",
       "      <th>-2.525521218776702881e-01</th>\n",
       "      <th>1.921887040138244629e+00</th>\n",
       "      <th>8.896374106407165527e-01</th>\n",
       "      <th>4.107718467712402344e-01</th>\n",
       "      <th>1.145620822906494141e+00</th>\n",
       "      <th>1.932632088661193848e+00</th>\n",
       "      <th>9.944640994071960449e-01</th>\n",
       "      <th>1.367815494537353516e+00</th>\n",
       "      <th>4.071449860930442810e-02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "      <td>4.999999e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.575655e-01</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>2.179504e-05</td>\n",
       "      <td>-5.018183e-05</td>\n",
       "      <td>9.994304e-01</td>\n",
       "      <td>-3.678343e-05</td>\n",
       "      <td>-1.954757e-05</td>\n",
       "      <td>9.999745e-01</td>\n",
       "      <td>3.545853e-05</td>\n",
       "      <td>1.001437e+00</td>\n",
       "      <td>-4.873422e-05</td>\n",
       "      <td>1.000360e+00</td>\n",
       "      <td>9.999559e-01</td>\n",
       "      <td>9.999165e-01</td>\n",
       "      <td>1.000421e+00</td>\n",
       "      <td>1.000112e+00</td>\n",
       "      <td>1.000192e+00</td>\n",
       "      <td>9.994851e-01</td>\n",
       "      <td>2.249139e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.981961e-01</td>\n",
       "      <td>6.873341e-01</td>\n",
       "      <td>1.003107e+00</td>\n",
       "      <td>1.001670e+00</td>\n",
       "      <td>6.542200e-01</td>\n",
       "      <td>1.002839e+00</td>\n",
       "      <td>1.001631e+00</td>\n",
       "      <td>8.728877e-01</td>\n",
       "      <td>1.001654e+00</td>\n",
       "      <td>8.901669e-01</td>\n",
       "      <td>1.001594e+00</td>\n",
       "      <td>6.288446e-01</td>\n",
       "      <td>5.841140e-01</td>\n",
       "      <td>4.708406e-01</td>\n",
       "      <td>8.592492e-01</td>\n",
       "      <td>6.207164e-01</td>\n",
       "      <td>6.237673e-01</td>\n",
       "      <td>4.360949e-01</td>\n",
       "      <td>1.969804e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.548815e-01</td>\n",
       "      <td>-2.102927e+00</td>\n",
       "      <td>-1.734789e+00</td>\n",
       "      <td>4.285860e-01</td>\n",
       "      <td>-2.059306e+00</td>\n",
       "      <td>-1.734202e+00</td>\n",
       "      <td>2.598711e-04</td>\n",
       "      <td>-1.727117e+00</td>\n",
       "      <td>7.693475e-08</td>\n",
       "      <td>-1.671863e+01</td>\n",
       "      <td>2.673070e-01</td>\n",
       "      <td>1.041228e-03</td>\n",
       "      <td>2.048078e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.734135e-02</td>\n",
       "      <td>4.452858e-03</td>\n",
       "      <td>3.211849e-07</td>\n",
       "      <td>4.172130e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.624837e-01</td>\n",
       "      <td>-7.573516e-01</td>\n",
       "      <td>-8.673567e-01</td>\n",
       "      <td>5.969881e-01</td>\n",
       "      <td>-7.695912e-01</td>\n",
       "      <td>-8.683741e-01</td>\n",
       "      <td>4.784546e-01</td>\n",
       "      <td>-8.661510e-01</td>\n",
       "      <td>3.691849e-01</td>\n",
       "      <td>-4.923983e-01</td>\n",
       "      <td>5.883285e-01</td>\n",
       "      <td>6.223411e-01</td>\n",
       "      <td>6.505438e-01</td>\n",
       "      <td>1.705213e-01</td>\n",
       "      <td>5.985209e-01</td>\n",
       "      <td>5.134394e-01</td>\n",
       "      <td>6.879128e-01</td>\n",
       "      <td>6.908195e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.915511e-01</td>\n",
       "      <td>1.339398e-04</td>\n",
       "      <td>-3.887097e-04</td>\n",
       "      <td>7.997993e-01</td>\n",
       "      <td>-3.792388e-04</td>\n",
       "      <td>2.009779e-04</td>\n",
       "      <td>7.738360e-01</td>\n",
       "      <td>-9.172366e-03</td>\n",
       "      <td>8.019273e-01</td>\n",
       "      <td>-8.037898e-02</td>\n",
       "      <td>8.286114e-01</td>\n",
       "      <td>8.781311e-01</td>\n",
       "      <td>9.344711e-01</td>\n",
       "      <td>9.018388e-01</td>\n",
       "      <td>8.355336e-01</td>\n",
       "      <td>9.142514e-01</td>\n",
       "      <td>1.094396e+00</td>\n",
       "      <td>1.672500e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.204413e+00</td>\n",
       "      <td>7.576488e-01</td>\n",
       "      <td>8.670981e-01</td>\n",
       "      <td>1.162353e+00</td>\n",
       "      <td>7.692302e-01</td>\n",
       "      <td>8.674778e-01</td>\n",
       "      <td>1.207444e+00</td>\n",
       "      <td>8.687620e-01</td>\n",
       "      <td>1.375392e+00</td>\n",
       "      <td>3.489010e-01</td>\n",
       "      <td>1.211083e+00</td>\n",
       "      <td>1.220313e+00</td>\n",
       "      <td>1.283522e+00</td>\n",
       "      <td>1.613231e+00</td>\n",
       "      <td>1.207994e+00</td>\n",
       "      <td>1.384583e+00</td>\n",
       "      <td>1.369183e+00</td>\n",
       "      <td>3.303655e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.055345e+01</td>\n",
       "      <td>2.101605e+00</td>\n",
       "      <td>1.734839e+00</td>\n",
       "      <td>3.303562e+01</td>\n",
       "      <td>2.059721e+00</td>\n",
       "      <td>1.734686e+00</td>\n",
       "      <td>2.106888e+01</td>\n",
       "      <td>1.740689e+00</td>\n",
       "      <td>2.338644e+01</td>\n",
       "      <td>2.048790e+01</td>\n",
       "      <td>2.107572e+01</td>\n",
       "      <td>1.616682e+01</td>\n",
       "      <td>6.731210e+00</td>\n",
       "      <td>2.068624e+01</td>\n",
       "      <td>2.115226e+01</td>\n",
       "      <td>1.561370e+01</td>\n",
       "      <td>1.591660e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0.000000000000000000e+00  9.728614687919616699e-01  \\\n",
       "count              4.999999e+06              4.999999e+06   \n",
       "mean               4.575655e-01              1.000318e+00   \n",
       "std                4.981961e-01              6.873341e-01   \n",
       "min                0.000000e+00              2.548815e-01   \n",
       "25%                0.000000e+00              5.624837e-01   \n",
       "50%                0.000000e+00              7.915511e-01   \n",
       "75%                1.000000e+00              1.204413e+00   \n",
       "max                1.000000e+00              2.055345e+01   \n",
       "\n",
       "       6.538545489311218262e-01  1.176224589347839355e+00  \\\n",
       "count              4.999999e+06              4.999999e+06   \n",
       "mean               2.179504e-05             -5.018183e-05   \n",
       "std                1.003107e+00              1.001670e+00   \n",
       "min               -2.102927e+00             -1.734789e+00   \n",
       "25%               -7.573516e-01             -8.673567e-01   \n",
       "50%                1.339398e-04             -3.887097e-04   \n",
       "75%                7.576488e-01              8.670981e-01   \n",
       "max                2.101605e+00              1.734839e+00   \n",
       "\n",
       "       1.157156467437744141e+00  -1.739873170852661133e+00  \\\n",
       "count              4.999999e+06               4.999999e+06   \n",
       "mean               9.994304e-01              -3.678343e-05   \n",
       "std                6.542200e-01               1.002839e+00   \n",
       "min                4.285860e-01              -2.059306e+00   \n",
       "25%                5.969881e-01              -7.695912e-01   \n",
       "50%                7.997993e-01              -3.792388e-04   \n",
       "75%                1.162353e+00               7.692302e-01   \n",
       "max                3.303562e+01               2.059721e+00   \n",
       "\n",
       "       -8.743090629577636719e-01  5.677649974822998047e-01  \\\n",
       "count               4.999999e+06              4.999999e+06   \n",
       "mean               -1.954757e-05              9.999745e-01   \n",
       "std                 1.001631e+00              8.728877e-01   \n",
       "min                -1.734202e+00              2.598711e-04   \n",
       "25%                -8.683741e-01              4.784546e-01   \n",
       "50%                 2.009779e-04              7.738360e-01   \n",
       "75%                 8.674778e-01              1.207444e+00   \n",
       "max                 1.734686e+00              2.106888e+01   \n",
       "\n",
       "       -1.750000417232513428e-01  8.100607395172119141e-01  \\\n",
       "count               4.999999e+06              4.999999e+06   \n",
       "mean                3.545853e-05              1.001437e+00   \n",
       "std                 1.001654e+00              8.901669e-01   \n",
       "min                -1.727117e+00              7.693475e-08   \n",
       "25%                -8.661510e-01              3.691849e-01   \n",
       "50%                -9.172366e-03              8.019273e-01   \n",
       "75%                 8.687620e-01              1.375392e+00   \n",
       "max                 1.740689e+00              2.338644e+01   \n",
       "\n",
       "       -2.525521218776702881e-01  1.921887040138244629e+00  \\\n",
       "count               4.999999e+06              4.999999e+06   \n",
       "mean               -4.873422e-05              1.000360e+00   \n",
       "std                 1.001594e+00              6.288446e-01   \n",
       "min                -1.671863e+01              2.673070e-01   \n",
       "25%                -4.923983e-01              5.883285e-01   \n",
       "50%                -8.037898e-02              8.286114e-01   \n",
       "75%                 3.489010e-01              1.211083e+00   \n",
       "max                 2.048790e+01              2.107572e+01   \n",
       "\n",
       "       8.896374106407165527e-01  4.107718467712402344e-01  \\\n",
       "count              4.999999e+06              4.999999e+06   \n",
       "mean               9.999559e-01              9.999165e-01   \n",
       "std                5.841140e-01              4.708406e-01   \n",
       "min                1.041228e-03              2.048078e-03   \n",
       "25%                6.223411e-01              6.505438e-01   \n",
       "50%                8.781311e-01              9.344711e-01   \n",
       "75%                1.220313e+00              1.283522e+00   \n",
       "max                1.616682e+01              6.731210e+00   \n",
       "\n",
       "       1.145620822906494141e+00  1.932632088661193848e+00  \\\n",
       "count              4.999999e+06              4.999999e+06   \n",
       "mean               1.000421e+00              1.000112e+00   \n",
       "std                8.592492e-01              6.207164e-01   \n",
       "min                0.000000e+00              2.734135e-02   \n",
       "25%                1.705213e-01              5.985209e-01   \n",
       "50%                9.018388e-01              8.355336e-01   \n",
       "75%                1.613231e+00              1.207994e+00   \n",
       "max                2.068624e+01              2.115226e+01   \n",
       "\n",
       "       9.944640994071960449e-01  1.367815494537353516e+00  \\\n",
       "count              4.999999e+06              4.999999e+06   \n",
       "mean               1.000192e+00              9.994851e-01   \n",
       "std                6.237673e-01              4.360949e-01   \n",
       "min                4.452858e-03              3.211849e-07   \n",
       "25%                5.134394e-01              6.879128e-01   \n",
       "50%                9.142514e-01              1.094396e+00   \n",
       "75%                1.384583e+00              1.369183e+00   \n",
       "max                1.561370e+01              1.591660e+00   \n",
       "\n",
       "       4.071449860930442810e-02  \n",
       "count              4.999999e+06  \n",
       "mean               2.249139e-01  \n",
       "std                1.969804e-01  \n",
       "min                4.172130e-08  \n",
       "25%                6.908195e-02  \n",
       "50%                1.672500e-01  \n",
       "75%                3.303655e-01  \n",
       "max                1.000000e+00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061873\n"
     ]
    }
   ],
   "source": [
    "# dataset['1.145620822906494141e+00']\n",
    "total = 0\n",
    "for x in range(len(dataset['1.145620822906494141e+00'])):\n",
    "    if dataset['1.145620822906494141e+00'][x] == 0:\n",
    "        total = total + 1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de Classificação\n",
    "\n",
    "A equipe utilizou como guia inicial para a escolha do algorítmo de classificação a documentação fornecida pelo sklearn, pois compõe uma relação compacta das variáveis a serem consideradas a cerca dos dados e tipo de classificação do problema quanto ao mesmo ser de aprendizado não supervisionado ([documentacao](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)). A imagem guia está disponível abaixo.\n",
    "\n",
    "![imgsklearn](https://preview.ibb.co/g4d0hS/sklearn.png)\n",
    "\n",
    "A premissa primeira engloba o fato de o problema ser de classificação, tal que os dados possuem labels bem definidas, binárias, que classificam uma partícula em exótica ou background. Tendo em vista o conjunto de 5.000.000 de dados na base de dados SUSY, foi preciso realizar a leitura de apenas uma parte dos mesmos, com o intuito principalmente de que os membros da equipe se tornassem familiarizados com o processo. Por esse motivo, foi utilizado inicialmente o KNN ou KNeighborsClassifier. \n",
    "\n",
    "A princípio mostrou resultados razoáveis, como é possível ver [aqui](#knnAcuracia), entretanto, o número de dados processados não englobava todos os registros do dataset e notou-se que esse método não era recomendado para conjunto de dados maiores que 100k. A incompatibilidade relatada, pode ser observada devido ao fato de que com acréscimos no número de registros a serem processados o algorítmo se tornava demasiadamente lento, como é possível observar na seção [Tempo de Execução](#tempoExecucao). É importante salientar que a análise desse algorítmo foi realizada pela equipe considerando apenas as últimas 8 colunas, pois a equipe raciocinou que, como elas eram funções das 10 primeiras poderia apresentar uma maior independência entre si, os resultados apresentados aqui, consideram essa condição. A partir de então, a equipe optou por testar [SGDClassifier], primeira opção para um conjunto de dados dessa dimensão, notando-se que o algorítmo não apresentou o resultado esperado o grupo optou por utilizar o Kernel Aproximate e o GBC.\n",
    "\n",
    "## Nearest Neighbors Classification\n",
    "\n",
    "A classificação baseada em vizinhos é um tipo de aprendizado que se baseia em intâncias : não tenta construir um modelo interno geral, mas simplesmente armazena instâncias dos dados de treinamento. A classificação é calculada a partir de uma votação simples dos vizinhos mais próximos de cada ponto.\n",
    "\n",
    "A classificação básica de vizinhos mais próximos usa pesos uniformes: o valor atribuído a um ponto é calculado a partir de uma votação simples por maioria  dos vizinhos mais próximos. Em algumas circunstâncias, é melhor ponderar os vizinhos de tal forma que os vizinhos mais próximos contribuam mais para o ajuste.\n",
    "\n",
    "[Documentação](http://scikit-learn.org/stable/modules/neighbors.html#classification)\n",
    "\n",
    "\n",
    "## KNeighborsClassifier\n",
    "\n",
    "É um algorítmo classificador que implementa o voto entre os k vizinhos mais próximos. \n",
    "\n",
    "[Documentação](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "\n",
    "## SGD - Stochastic Gradient Descent\n",
    "\n",
    "O Algorítmo SGD é uma abordagem simples porém eficiente, sendo esta abordagem, facilmente implementada. A abordagem SGD é utilizada para modelos discriminativos com classificadores lineares sobre funções convexas de perda, tais como a regressão linear.Bastante aplicado em aprendizado de larga escala, o SGDClassifier é a alternativa para bases de dados em que o KNeighborsClassifier se mostra ineficiente.\n",
    "\n",
    "Por trabalhar com bases de dados de grande volume, o SGD necessita de hiper parametros, ou seja, necessita que sejam analisadas as features mais relevantes. Isso se alcança com uma boa feature selection antes de se aplicar o SGD Classifier.\n",
    "\n",
    "[Documentação](http://scikit-learn.org/stable/modules/sgd.html)\n",
    "\n",
    "\n",
    "## SGDClassifier\n",
    "\n",
    "É um algorítmo classificador que implementa modelos lineares utilizando curvas de gradiente.\n",
    "\n",
    "[documentacao](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    "\n",
    "\n",
    "##  Gradient Tree Boosting\n",
    "\n",
    "O Gradient Boosting ou Gradient Boosted Regression Trees (GBRT) é uma generalização de estímulo/impulsionamento para funções de perda arbitrariamente diferenciáveis. O GBRT é um procedimento pronto e efetivo que pode ser usado tanto para problemas de regressão quanto de classificação. Modelos de Gradient Tree Boosting são usados ​​em uma variedade de áreas, incluindo ranking de busca na Web e ecologia.\n",
    "\n",
    "\n",
    "## GradientBoostingClassifier\n",
    "\n",
    "É usado em problemas de classificação e suporta classificação binária e multi-classe. \n",
    "\n",
    "O número de apredizes fracos ou árvores de regressão pode ser controlado pelo parâmetro n_estimators, além disso, o tamanho de cada árvore é definido pela profundidade da árvore via max_depth definindo o número de nós de folha via max_leaf_nodes. O learning_rate é um hiper-parâmetro no intervalo (0.0, 1.0) que controla o overfitting através do encolhimento .\n",
    "\n",
    "[GradientBoosting](http://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)\n",
    "\n",
    "\n",
    "KNN,SGD,KA,GBC colocar imagem do sklearnin de seleção dos algoritmos  \n",
    "Rafael, Adrianne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise das Features\n",
    "\n",
    "Gesiel falar sobre as variações dos modelos e pré concluir.\n",
    "\n",
    "## Pré Processamento\n",
    "\n",
    "    O pré processamento de dados é uma fase no aprendizado de maquina que busca avaliar a qualidade dos dados e suas relações, e se iniciar logo depois de os dados terem sido coletados e organizados em um conjunto de dados. Essa fase se dá pelo fato de existirem diversos problemas nos dados que precisam ser solucionados como por exemplo: dados faltando, dados corrompidos, dados irrelevantes e valores desconhecidos.\n",
    "    \n",
    "    Uma caracteristica que os algoritmos de AM, vem assumindo é determinar que os dados já vem pré processados, isso porque a comunidade de AM usam os repositorios de dados para obter conjuntos de dados para utilização e por essa razão não apresentam varios problemas que podem ser encontrados em dados coletados no mundo real e acabam não realizando a fase de pré processamento.\n",
    "    \n",
    "    Diante desde dilema e pelo fato de o dataset SUSY possuir um conjunto de dados de cinco milhoes de registro e 18 variaveis independentes, esta sessão possue por objetivo buscar reduzir as variaveis independentes(features) do dataset, afim de melhorar o processamento e os resultados obtidos após o processamento e analise dos resultados, dessa forma varias tecnicas de redução de featues juntamente com analise gráfica seram utiladas em conjunto para determinar se é possivel reduzir as features.\n",
    "    \n",
    "    Este objetivo foi proposto depois de uma analise da estrutura dos dados e visualização dos mesmos utilizando tecnicas de visualização de dados, foi observado que os dados já foram pré processados como pode ser observado: não existem dados faltando, não existem dados corrompidos e não existem valores corrompidos.\n",
    "    \n",
    "    Analisando dois trabalhos feitos utilizando este mesmo conjunto de dados(SUSY) disponíveis em: [Searching for Exotic Particles in ...](https://arxiv.org/pdf/1402.4735.pdf) e [Modelo preditivo de Susy ...](http://machinelearningandspark.blogspot.com.br/2016/07/susy-dataset.html), notou-se que os melhores resultados foram obtidos utilizando todas as features\n",
    "    \n",
    "    \n",
    "## Redução de Features\n",
    "\n",
    "### Correlação\n",
    "\n",
    "A matriz de correlação é composta pelos coeficientes de correlação (por padrão, o coeficiente de Pearson) entre cada par de colunas. Um alto coeficiente indica um par de colunas que tendem a conter informação similar, ou até mesmo redundante. A seleção de colunas por filtragem de alta correlação depende da definição de um limiar de correlação máxima - ou threshold. A escolha deste limiar, em geral, é feita por tentativa e erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16a31464710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16a30fb1550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(dataset.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Seleção Univariada\n",
    "\n",
    "Este método realiza testes estatísticos individualmente em cada coluna, e calcula um score. Os scores mais altos podem ser então selecionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.829e+05 5.370e-01 1.065e+00 1.972e+05 7.207e-01 4.910e-02 1.192e+06\n",
      " 6.008e-02 4.346e+05 2.988e+04 3.825e+05 1.090e+06 6.344e+04 3.174e+04\n",
      " 3.575e+05 4.026e+05 5.338e+03 3.893e+05]\n",
      "[[1.668 3.475 0.013 1.046 0.568 0.448 0.205 0.378]\n",
      " [0.445 1.22  1.831 0.526 0.942 0.603 1.562 0.181]\n",
      " [0.381 2.033 3.046 0.569 1.015 0.761 1.715 0.091]\n",
      " [1.31  1.088 0.589 1.179 0.968 1.083 0.043 0.095]\n",
      " [0.456 0.323 0.17  0.489 0.416 0.461 0.346 0.186]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "array = dataset.values\n",
    "X = array[:, 1:]\n",
    "Y = array[:, 0]\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=f_classif, k=8)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "features = fit.transform(X)\n",
    "\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "\n",
    "A Recursive Feature Elimination (ou RFE) consiste na remoção de maneira recursiva de atributos e construção de modelo com os que permanecem. De maneira geral, utiliza a precisão de um modelo para identificar quais atributos são mais relevantes para a predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 18 artists>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16a31684898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 8)\n",
    "fit = rfe.fit(X, Y)\n",
    "\n",
    "plt.bar(range(18), fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "A Análise de Componentes Principais, ou Principal Component Analysis (PCA), é um procedimento estatístico destinado à redução de dimensionalidade. Ele transforma um conjunto de variáveis independentes em um conjunto menor ou igual a partir da variância. É calculado um índice que ordena as colunas, sendo a primeira coluna selecionada, ou seja, o primeiro componente principal, o que apresenta a maior variância. Os componentes seguintes são ordenados também por variância, mas com a restrição de que sejam ortogonais aos anteriores, condição esta que garante que sejam independente, ou não correlacionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.215 0.147 0.126]\n",
      "[[ 2.174e-01 -7.178e-04  2.596e-03  1.082e-01 -4.218e-04  3.437e-04\n",
      "   4.145e-01 -2.391e-03  4.744e-01 -2.552e-01  1.629e-01  3.333e-01\n",
      "   1.723e-01  3.729e-01  1.905e-01  3.563e-01  8.061e-02  5.369e-02]\n",
      " [ 3.917e-01 -3.124e-03 -2.573e-03  3.871e-01 -3.265e-03  9.064e-04\n",
      "   1.631e-01 -2.571e-03 -1.102e-01  3.968e-01  3.961e-01  1.332e-01\n",
      "  -2.120e-01 -3.647e-01  3.627e-01 -1.166e-01 -4.093e-02 -3.339e-02]\n",
      " [ 2.105e-03  7.073e-01  4.551e-03  1.767e-03  7.069e-01 -2.330e-03\n",
      "   1.160e-03 -3.037e-03 -1.561e-04  1.281e-03  1.877e-03  9.714e-04\n",
      "  -6.716e-04 -1.607e-03  1.759e-03 -5.339e-04 -6.984e-05  9.038e-05]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Além dos métodos apresentados anteriormente, é possível utilizar árvore de decisão (Random Forest e Extra Trees) para estimar a importância das features. Abaixo tem um exemplo de aplicação de ExtraTrees, em que se constrói um modelo e a partir dele é calculada uma importância para cada feature. Nesse caso, pode-se dizer que class, lepton 2 phi e M_R são os três elementos mais importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 18 artists>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16a319a1f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "plt.bar(range(18), model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste Redução\n",
    "    Testar as features com os algoritmos SGD, GBC de classificação\n",
    "    Afonso\n",
    "## Conclusão Análise Features\n",
    "    Afonso\n",
    "Concluir que é melhor não reduzir as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "## Algoritmo utilizado\n",
    "Para a Solução do Problema, a equipe fez o uso de 3 algorítmos diferentes. \n",
    "\n",
    "A primeira abordagem que tomamos para a resolução do problema, foi implementar o *Nearest Neighbors Classification*. Durante a aplicação do algorítmo KNN, observamos certa demora na execução do código, entretanto, ao final do teste, foi possível obter, com 17 vizinhos e 500.000 amostras, o resultado ótimo para o algorítmo, que foi uma acurácia de 0.78437066, obtida após a feature selection, sendo a acurácia, utilizando todas as features, equivalente a cerca de 0.76. \n",
    "\n",
    "Entretanto, após consulta do site da biblioteca Sklearn e também análise da ineficiência do algoritmo, por parte da equipe, concluiu-se que o uso do algorítmo KNN era inedequado para uma base de dados da dimensão da nossa, visto que, este algorítmo, é adequado para um máximo de 100.000 amostras e nossa base tinha 5.000.000.\n",
    "\n",
    "Visto a ineficiência do KNN, apesar do resultado razoável, decidimos fazer uso do *Stochastic Gradient Descent*. Com o uso do algorítmo SGD, além de reduzir drasticamente o tempo de execução do algorítmo, reduzindo o tempo de varios minutos para segundos, foi possível obter um resultado melhor ainda! A acurácia aumentou para 0.7884, utilizando todas as features do dataset, apesar do resultado, não foram feitas alterações posteriores ou a aplicação da feature selection com o SGD.\n",
    "\n",
    "Apesar de já ter obtido bons resultados utilizando o SGD, a equipe também implementou o *Gradient Boosting* ou *Gradient Boosted Regression Trees Classifier*. O algorítmo GBRT melhorou ainda mais o resultado obtido pela equipe, tendo, ao final, com as 5 milhões de amostras, dataset de treino de 75% e min_samples_split de 700, obter o melhor resultado. Sendo esta, uma acurácia total e final de 0.8031416 nos testes.\n",
    "\n",
    "## Resultados Obtidos\n",
    "Thiago\n",
    "    Apresentar resultados obtidos, gráficos e resultados da mensuração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "BATISTA, Gustavo Enrique de Almeida Prado Alves. Pré-processamento de dados em aprendizado de máquina supervisionado. 2003. Tese (Doutorado em Ciências de Computação e Matemática Computacional) - Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, 2003. doi:10.11606/T.55.2003.tde-06102003-160219. Acesso em: 2018-04-16."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
